"""
ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ë° ìš”ì•½ ë„êµ¬
Claude ìˆ˜ì¤€ì˜ ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ ì œê³µ
"""

import logging
import os
import asyncio
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from langchain.tools import BaseTool
from pydantic import Field
from openai import OpenAI
import json
import re

from langchain_service.core.database_manager import db_manager, NewsArticle

logger = logging.getLogger(__name__)

class AdvancedNewsAnalyzer(BaseTool):
    """ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ë„êµ¬ - Claude ìˆ˜ì¤€ì˜ ë¶„ì„"""
    
    name: str = "advanced_news_analyzer"
    description: str = """
    ì•”í˜¸í™”í ë‰´ìŠ¤ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ Claude ìˆ˜ì¤€ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    íŠ¹ì§•:
    - ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„
    - ê°ì • ë¶„ì„ ë° ì‹œì¥ ì‹¬ë¦¬ íŒŒì•…
    - ì£¼ìš” ì´ë²¤íŠ¸ì™€ ì˜í–¥ë„ í‰ê°€
    - ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í•´ì„ ì œê³µ
    - íˆ¬ì ê´€ì ì—ì„œì˜ ì‹œì‚¬ì  ë¶„ì„
    
    ì‚¬ìš© ì˜ˆì‹œ:
    - "ë¹„íŠ¸ì½”ì¸ ìµœì‹  ë™í–¥ ë¶„ì„í•´ì¤˜"
    - "ì•”í˜¸í™”í ì‹œì¥ ì‹¬ë¦¬ëŠ” ì–´ë•Œ?"
    - "ETF ìŠ¹ì¸ì´ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"
    """
    
    def __init__(self):
        super().__init__()
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
    def _run(self, query: str) -> str:
        """ë¹„ë™ê¸° ì‹¤í–‰ì„ ë™ê¸°ë¡œ ë˜í•‘"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._arun(query))
        finally:
            loop.close()
    
    async def _arun(self, query: str) -> str:
        """ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ” ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {query}")
            
            # 1ë‹¨ê³„: ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
            news_articles = await self._collect_relevant_news(query)
            
            if not news_articles:
                return "ğŸ“° í˜„ì¬ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ë³´ì„¸ìš”."
            
            # 2ë‹¨ê³„: ë‰´ìŠ¤ ë‚´ìš© ì‹¬ì¸µ ë¶„ì„
            analysis_result = await self._analyze_news_content(news_articles, query)
            
            # 3ë‹¨ê³„: ì‹œì¥ ë°ì´í„°ì™€ ì—°ê³„ ë¶„ì„
            market_context = await self._get_market_context()
            
            # 4ë‹¨ê³„: Claude ìŠ¤íƒ€ì¼ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
            comprehensive_report = await self._generate_comprehensive_report(
                query, news_articles, analysis_result, market_context
            )
            
            logger.info(f"âœ… ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(comprehensive_report)}ì")
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _collect_relevant_news(self, query: str, limit: int = 10) -> List[NewsArticle]:
        """ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        # í‚¤ì›Œë“œ í™•ì¥
        expanded_queries = await self._expand_search_keywords(query)
        
        all_articles = []
        for expanded_query in expanded_queries:
            articles = await db_manager.search_news_advanced(
                query=expanded_query,
                limit=limit // len(expanded_queries) + 1,
                use_vector_search=True,
                similarity_threshold=0.2
            )
            all_articles.extend(articles)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ìˆœ ì •ë ¬
        unique_articles = []
        seen_titles = set()
        
        for article in all_articles:
            title_key = article.title.lower().strip()
            if title_key not in seen_titles and len(title_key) > 10:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        # ìµœì‹ ìˆœ + ê´€ë ¨ë„ìˆœ ì •ë ¬
        unique_articles.sort(key=lambda x: (x.relevance_score, x.published_date or datetime.min), reverse=True)
        
        return unique_articles[:limit]
    
    async def _expand_search_keywords(self, query: str) -> List[str]:
        """ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥"""
        base_queries = [query]
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        if any(keyword in query.lower() for keyword in ['ë¹„íŠ¸ì½”ì¸', 'bitcoin', 'btc']):
            base_queries.extend([
                'bitcoin price analysis',
                'btc market trend',
                'bitcoin institutional adoption',
                'ë¹„íŠ¸ì½”ì¸ ì‹œì¥ ë¶„ì„',
                'ë¹„íŠ¸ì½”ì¸ ì œë„ê¶Œ ì§„ì…'
            ])
        
        # ETF ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        if 'etf' in query.lower():
            base_queries.extend([
                'bitcoin etf approval',
                'crypto etf impact',
                'institutional crypto investment'
            ])
        
        # ê·œì œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        if any(keyword in query.lower() for keyword in ['ê·œì œ', 'regulation', 'sec']):
            base_queries.extend([
                'crypto regulation news',
                'sec bitcoin decision',
                'cryptocurrency legal framework'
            ])
        
        return base_queries[:5]  # ìµœëŒ€ 5ê°œ ì¿¼ë¦¬
    
    async def _analyze_news_content(self, articles: List[NewsArticle], user_query: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë‚´ìš© ì‹¬ì¸µ ë¶„ì„"""
        if not articles:
            return {}
        
        # ë‰´ìŠ¤ ë‚´ìš© ì¤€ë¹„
        news_content = []
        for i, article in enumerate(articles[:5], 1):  # ìƒìœ„ 5ê°œ ê¸°ì‚¬ë§Œ
            content = f"{i}. **{article.title}**\n"
            content += f"   ì¶œì²˜: {article.source}\n"
            content += f"   ë‚ ì§œ: {article.published_date}\n"
            if article.summary:
                content += f"   ìš”ì•½: {article.summary}\n"
            news_content.append(content)
        
        analysis_prompt = f"""ë‹¤ìŒ ì•”í˜¸í™”í ë‰´ìŠ¤ë“¤ì„ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ì‹¬ì¸µ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}"

ë‰´ìŠ¤ ìë£Œ:
{chr(10).join(news_content)}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. **ì‹œì¥ íŠ¸ë Œë“œ íŒŒì•…**: í˜„ì¬ ì‹œì¥ì˜ ì£¼ìš” íë¦„ê³¼ ë°©í–¥ì„±
2. **ê°ì • ë¶„ì„**: ì‹œì¥ ì°¸ì—¬ìë“¤ì˜ ì‹¬ë¦¬ ìƒíƒœ (ê°•ì„¸/ì•½ì„¸/ê´€ë§)
3. **í•µì‹¬ ì´ë²¤íŠ¸**: ê°€ê²©ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì£¼ìš” ì‚¬ê±´ë“¤
4. **ë¦¬ìŠ¤í¬ ìš”ì¸**: ì ì¬ì  ìœ„í—˜ ìš”ì†Œë“¤
5. **ê¸°íšŒ ìš”ì¸**: ê¸ì •ì  ì„±ì¥ ë™ë ¥ë“¤

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{
    "market_trend": "í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„",
    "sentiment_analysis": "ì‹œì¥ ê°ì • ë¶„ì„",
    "key_events": ["ì£¼ìš” ì´ë²¤íŠ¸ 1", "ì£¼ìš” ì´ë²¤íŠ¸ 2"],
    "risk_factors": ["ë¦¬ìŠ¤í¬ ìš”ì¸ 1", "ë¦¬ìŠ¤í¬ ìš”ì¸ 2"],
    "opportunity_factors": ["ê¸°íšŒ ìš”ì¸ 1", "ê¸°íšŒ ìš”ì¸ 2"],
    "overall_assessment": "ì¢…í•© í‰ê°€"
}}"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",  # ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ GPT-4 ì‚¬ìš©
                messages=[{"role": "user", "content": analysis_prompt}],
                max_tokens=1500,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ë¶„ì„
                return {"raw_analysis": result_text}
                
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _get_market_context(self) -> Dict[str, Any]:
        """ì‹œì¥ ë§¥ë½ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ
            db_stats = await db_manager.get_database_stats()
            
            # ìµœê·¼ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„
            recent_news = await db_manager.get_recent_news(hours=24, limit=20)
            
            # ë‰´ìŠ¤ ë¹ˆë„ ë¶„ì„
            news_frequency = {
                '24h': len([n for n in recent_news if n.published_date and 
                           (datetime.now() - n.published_date).total_seconds() < 86400]),
                '7d': db_stats.get('news_7d', 0)
            }
            
            return {
                'database_stats': db_stats,
                'news_frequency': news_frequency,
                'data_freshness': db_stats.get('last_updated'),
                'total_articles_analyzed': len(recent_news)
            }
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ë§¥ë½ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _generate_comprehensive_report(
        self, 
        query: str, 
        articles: List[NewsArticle], 
        analysis: Dict[str, Any], 
        market_context: Dict[str, Any]
    ) -> str:
        """Claude ìŠ¤íƒ€ì¼ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        current_time = datetime.now().strftime("%H:%M")
        
        # ê¸°ì‚¬ ìš”ì•½ ìƒì„±
        top_articles = articles[:3]
        article_summaries = []
        for i, article in enumerate(top_articles, 1):
            summary = f"{i}. **{article.title}**"
            if article.published_date:
                date_str = article.published_date.strftime("%m/%d %H:%M")
                summary += f" ({date_str})"
            if article.summary:
                summary += f"\n   {article.summary[:100]}..."
            summary += f"\n   ğŸ“Š ê´€ë ¨ë„: {article.relevance_score:.2f} | ì¶œì²˜: {article.source}"
            article_summaries.append(summary)
        
        # ë¶„ì„ ê²°ê³¼ ì •ë¦¬
        market_trend = analysis.get('market_trend', 'ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„° ë¶€ì¡±')
        sentiment = analysis.get('sentiment_analysis', 'ì‹œì¥ ê°ì • ë°ì´í„° ë¶€ì¡±')
        key_events = analysis.get('key_events', [])
        risks = analysis.get('risk_factors', [])
        opportunities = analysis.get('opportunity_factors', [])
        
        # ë°ì´í„° ì‹ ë¢°ë„ í‘œì‹œ
        total_articles = market_context.get('database_stats', {}).get('total_news', 0)
        recent_articles = market_context.get('news_frequency', {}).get('24h', 0)
        
        comprehensive_report = f"""# ğŸ” ì•”í˜¸í™”í ì‹œì¥ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ê¸°ì¤€ì‹œì **: {current_date} {current_time}  
**ë¶„ì„ ëŒ€ìƒ**: "{query}"  
**ë°ì´í„° ê¸°ë°˜**: ì´ {total_articles}ê°œ ê¸°ì‚¬ (ìµœê·¼ 24ì‹œê°„ {recent_articles}ê°œ)

---

## ğŸ“Š **í•µì‹¬ ì‹œì¥ ë™í–¥**

{market_trend}

### ğŸ¯ **ì‹œì¥ ì‹¬ë¦¬ ë¶„ì„**
{sentiment}

---

## ğŸ“° **ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„**

{chr(10).join(article_summaries)}

---

## âš¡ **ì£¼ìš” ì´ë²¤íŠ¸ & ì´‰ë§¤**

{chr(10).join([f"â€¢ **{event}**" for event in key_events]) if key_events else "â€¢ í˜„ì¬ íŠ¹ë³„í•œ ì‹œì¥ ì´ë²¤íŠ¸ëŠ” ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

---

## âš ï¸ **ë¦¬ìŠ¤í¬ ìš”ì¸**

{chr(10).join([f"â€¢ {risk}" for risk in risks]) if risks else "â€¢ í˜„ì¬ ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì´ ëª…í™•íˆ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

---

## ğŸš€ **ê¸°íšŒ ìš”ì¸**

{chr(10).join([f"â€¢ {opp}" for opp in opportunities]) if opportunities else "â€¢ í˜„ì¬ ëª…í™•í•œ ê¸°íšŒ ìš”ì¸ì´ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

---

## ğŸ¯ **ì¢…í•© í‰ê°€ ë° ì „ë§**

{analysis.get('overall_assessment', 'í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ì¢…í•©í•´ë³´ë©´, ì‹œì¥ì€ ì—¬ëŸ¬ ìš”ì¸ë“¤ì˜ ì˜í–¥ì„ ë°›ê³  ìˆìœ¼ë©°, íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.')}

---

## ğŸ“‹ **ë°ì´í„° ì‹ ë¢°ë„**

- **ë¶„ì„ ê¸°ì‚¬ ìˆ˜**: {len(articles)}ê°œ
- **ë²¡í„° ê²€ìƒ‰ ì •í™•ë„**: í‰ê·  {sum(a.relevance_score for a in articles) / len(articles):.2f}
- **ë°ì´í„° ì‹ ì„ ë„**: ìµœê·¼ 24ì‹œê°„ ë‚´ {recent_articles}ê°œ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸
- **ì„ë² ë”© ì²˜ë¦¬ìœ¨**: {market_context.get('database_stats', {}).get('embedding_coverage', 0):.1f}%

*ğŸ’¡ ì´ ë¶„ì„ì€ AIê°€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•œ ê²ƒìœ¼ë¡œ, íˆ¬ì ê²°ì • ì‹œ ì¶”ê°€ì ì¸ ì •ë³´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.*"""

        return comprehensive_report

class MarketSentimentAnalyzer(BaseTool):
    """ì‹œì¥ ê°ì • ë¶„ì„ ì „ìš© ë„êµ¬"""
    
    name: str = "market_sentiment_analyzer"
    description: str = """
    ì•”í˜¸í™”í ì‹œì¥ì˜ ì „ë°˜ì ì¸ ê°ì •ê³¼ ì‹¬ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    íŠ¹ì§•:
    - ë‰´ìŠ¤ ê¸°ë°˜ ê°ì • ë¶„ì„
    - ì‹œì¥ ì°¸ì—¬ì ì‹¬ë¦¬ íŒŒì•…
    - Fear & Greed ì§€ìˆ˜ í•´ì„
    - íˆ¬ìì í–‰ë™ íŒ¨í„´ ë¶„ì„
    """
    
    def __init__(self):
        super().__init__()
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    def _run(self, query: str = "ì‹œì¥ ê°ì • ë¶„ì„") -> str:
        """ì‹œì¥ ê°ì • ë¶„ì„ ì‹¤í–‰"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._analyze_market_sentiment())
        finally:
            loop.close()
    
    async def _analyze_market_sentiment(self) -> str:
        """ì‹œì¥ ê°ì • ë¶„ì„"""
        try:
            logger.info("ğŸ“Š ì‹œì¥ ê°ì • ë¶„ì„ ì‹œì‘")
            
            # ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
            recent_news = await db_manager.get_recent_news(hours=24, limit=15)
            
            if not recent_news:
                return "ğŸ“Š ìµœê·¼ 24ì‹œê°„ ë‚´ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•˜ì—¬ ì •í™•í•œ ì‹œì¥ ê°ì • ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤."
            
            # ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ìœ¼ë¡œ ê°ì • ë¶„ì„
            news_texts = []
            for article in recent_news:
                text = f"{article.title}"
                if article.summary:
                    text += f" {article.summary}"
                news_texts.append(text)
            
            sentiment_prompt = f"""ë‹¤ìŒ ìµœê·¼ 24ì‹œê°„ ì•”í˜¸í™”í ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ì‹œì¥ ê°ì •ì„ í‰ê°€í•´ì£¼ì„¸ìš”:

ë‰´ìŠ¤ ë°ì´í„°:
{chr(10).join([f"- {text[:150]}..." for text in news_texts])}

ë¶„ì„ ê¸°ì¤€:
1. ì „ë°˜ì ì¸ ì‹œì¥ ê°ì • (ë§¤ìš° ë¶€ì •ì  ~ ë§¤ìš° ê¸ì •ì )
2. íˆ¬ìì ì‹¬ë¦¬ ìƒíƒœ
3. ì£¼ìš” ê°ì • ë™ì¸ (ë¬´ì—‡ì´ ê°ì •ì„ ì£¼ë„í•˜ëŠ”ê°€)
4. ë‹¨ê¸° ì „ë§ (í–¥í›„ 1-7ì¼)

ì‹œì¥ ê°ì •ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

ğŸ“Š **ì‹œì¥ ê°ì • ì§€ìˆ˜**: [ì ìˆ˜]/10 (1=ê·¹ë„ ê³µí¬, 10=ê·¹ë„ íƒìš•)

ğŸ­ **í˜„ì¬ ì‹¬ë¦¬ ìƒíƒœ**: 
[ìƒì„¸ ë¶„ì„]

âš¡ **ì£¼ìš” ê°ì • ë™ì¸**:
â€¢ [ë™ì¸ 1]
â€¢ [ë™ì¸ 2] 
â€¢ [ë™ì¸ 3]

ğŸ”® **ë‹¨ê¸° ì „ë§**:
[í–¥í›„ ì „ë§]

ğŸ“ˆ **íˆ¬ìì í–‰ë™ ê¶Œê³ **:
[í–‰ë™ ê°€ì´ë“œ]"""

            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": sentiment_prompt}],
                max_tokens=800,
                temperature=0.2
            )
            
            result = response.choices[0].message.content.strip()
            
            # ë¶„ì„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M")
            footer = f"\n\n---\n*ğŸ“… ë¶„ì„ ì‹œì : {current_time} | ğŸ“° ë¶„ì„ ê¸°ì‚¬: {len(recent_news)}ê°œ*"
            
            return result + footer
            
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"ì‹œì¥ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class TrendAnalyzer(BaseTool):
    """íŠ¸ë Œë“œ ë¶„ì„ ë„êµ¬"""
    
    name: str = "trend_analyzer"  
    description: str = """
    ì•”í˜¸í™”í ì‹œì¥ì˜ íŠ¸ë Œë“œì™€ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    íŠ¹ì§•:
    - ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„
    - í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
    - ì´ìŠˆì˜ ìƒëª…ì£¼ê¸° ì¶”ì 
    - ìƒˆë¡œìš´ íŠ¸ë Œë“œ ë°œêµ´
    """
    
    def _run(self, period: str = "7d") -> str:
        """íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._analyze_trends(period))
        finally:
            loop.close()
    
    async def _analyze_trends(self, period: str) -> str:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            logger.info(f"ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {period}")
            
            # ê¸°ê°„ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
            hours_map = {"24h": 24, "7d": 168, "30d": 720}
            hours = hours_map.get(period, 168)
            
            news_articles = await db_manager.get_recent_news(hours=hours, limit=50)
            
            if not news_articles:
                return f"ğŸ“ˆ ìµœê·¼ {period} ê¸°ê°„ì˜ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•˜ì—¬ íŠ¸ë Œë“œ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤."
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
            keyword_analysis = self._extract_trending_keywords(news_articles)
            
            # ì‹œê°„ëŒ€ë³„ ë¶„í¬ ë¶„ì„
            temporal_analysis = self._analyze_temporal_distribution(news_articles)
            
            # íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„±
            report = f"""# ğŸ“ˆ ì•”í˜¸í™”í ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ({period})

## ğŸ”¥ **í•« í‚¤ì›Œë“œ TOP 10**
{chr(10).join([f"{i}. **{keyword}** ({count}íšŒ ì–¸ê¸‰)" for i, (keyword, count) in enumerate(keyword_analysis[:10], 1)])}

## â° **ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ë¶„í¬**
- ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(news_articles)}ê°œ
- ì¼í‰ê·  ë‰´ìŠ¤: {len(news_articles) / (hours / 24):.1f}ê°œ
- ìµœì‹  ë‰´ìŠ¤: {temporal_analysis.get('latest', 'ì •ë³´ ì—†ìŒ')}

## ğŸ“Š **íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸**
{self._generate_trend_insights(keyword_analysis, temporal_analysis)}

---
*ğŸ“… ë¶„ì„ ê¸°ê°„: ìµœê·¼ {period} | ë¶„ì„ ê¸°ì‚¬: {len(news_articles)}ê°œ*"""

            return report
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_trending_keywords(self, articles: List[NewsArticle]) -> List[Tuple[str, int]]:
        """íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_count = {}
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ëª©ë¡
        important_keywords = [
            'bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'ethereum', 'eth', 'ì´ë”ë¦¬ì›€',
            'etf', 'sec', 'ê·œì œ', 'regulation', 'trump', 'íŠ¸ëŸ¼í”„',
            'mining', 'ì±„êµ´', 'halving', 'ë°˜ê°ê¸°', 'institutional',
            'adoption', 'ì œë„ê¶Œ', 'whale', 'ê³ ë˜', 'defi', 'nft'
        ]
        
        for article in articles:
            text = (article.title + " " + (article.summary or "")).lower()
            
            for keyword in important_keywords:
                count = text.count(keyword.lower())
                if count > 0:
                    keyword_count[keyword] = keyword_count.get(keyword, 0) + count
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        return sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
    
    def _analyze_temporal_distribution(self, articles: List[NewsArticle]) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ ë¶„í¬ ë¶„ì„"""
        if not articles:
            return {}
        
        # ìµœì‹  ê¸°ì‚¬ ì°¾ê¸°
        latest_article = max(articles, key=lambda x: x.published_date or datetime.min)
        
        # ë‚ ì§œë³„ ë¶„í¬
        daily_counts = {}
        for article in articles:
            if article.published_date:
                date_key = article.published_date.strftime("%Y-%m-%d")
                daily_counts[date_key] = daily_counts.get(date_key, 0) + 1
        
        return {
            'latest': latest_article.published_date.strftime("%m/%d %H:%M") if latest_article.published_date else None,
            'daily_distribution': daily_counts,
            'peak_day': max(daily_counts.keys(), key=lambda x: daily_counts[x]) if daily_counts else None
        }
    
    def _generate_trend_insights(self, keywords: List[Tuple[str, int]], temporal: Dict[str, Any]) -> str:
        """íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        if keywords:
            top_keyword = keywords[0][0]
            insights.append(f"â€¢ í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ëŠ” í‚¤ì›Œë“œëŠ” '{top_keyword}'ì…ë‹ˆë‹¤.")
        
        if temporal.get('peak_day'):
            insights.append(f"â€¢ {temporal['peak_day']}ì— ê°€ì¥ ë§ì€ ë‰´ìŠ¤ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # ETF ê´€ë ¨ íŠ¸ë Œë“œ
        etf_mentions = sum(count for keyword, count in keywords if 'etf' in keyword.lower())
        if etf_mentions > 0:
            insights.append(f"â€¢ ETF ê´€ë ¨ ì´ìŠˆê°€ {etf_mentions}íšŒ ì–¸ê¸‰ë˜ì–´ ë†’ì€ ê´€ì‹¬ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
        
        # ê·œì œ ê´€ë ¨ íŠ¸ë Œë“œ  
        reg_mentions = sum(count for keyword, count in keywords if keyword.lower() in ['sec', 'ê·œì œ', 'regulation'])
        if reg_mentions > 0:
            insights.append(f"â€¢ ê·œì œ ê´€ë ¨ ë‰´ìŠ¤ê°€ {reg_mentions}íšŒ ì–¸ê¸‰ë˜ì–´ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return "\n".join(insights) if insights else "â€¢ í˜„ì¬ íŠ¹ë³„í•œ íŠ¸ë Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."