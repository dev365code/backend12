"""
í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì
ì—°ê²° í’€ë§, ìºì‹±, ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ í†µí•© ê´€ë¦¬
"""

import asyncio
import logging
import os
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
import asyncpg
import redis
import json
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ëª¨ë¸"""
    id: Optional[int] = None
    title: str = ""
    summary: str = ""
    content: str = ""
    url: str = ""
    source: str = ""
    published_date: datetime = None
    keywords: List[str] = None
    sentiment: str = "neutral"
    relevance_score: float = 0.0
    embedding: List[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        if self.published_date:
            data['published_date'] = self.published_date.isoformat()
        return data

class DatabaseManager:
    """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pgvector_pool: Optional[asyncpg.Pool] = None
        self.postgres_pool: Optional[asyncpg.Pool] = None
        self.redis_client: Optional[redis.Redis] = None
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ
        
        # DB ì„¤ì •
        self.pgvector_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5435)),
            'database': os.getenv('DB_NAME', 'mydb'),
            'user': os.getenv('DB_USER', 'myuser'),
            'password': os.getenv('DB_PASSWORD', 'mypassword'),
            'min_size': 5,
            'max_size': 20
        }
        
        self.postgres_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'database': os.getenv('POSTGRES_DB', 'postgres'),
            'user': os.getenv('POSTGRES_USER', 'leewooyong'),
            'password': os.getenv('POSTGRES_PASSWORD', ''),
            'min_size': 5,
            'max_size': 20
        }
        
    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
            
            # PgVector ì—°ê²° í’€ ìƒì„±
            self.pgvector_pool = await asyncpg.create_pool(**self.pgvector_config)
            logger.info("âœ… PgVector ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
            
            # PostgreSQL ì—°ê²° í’€ ìƒì„±
            try:
                self.postgres_pool = await asyncpg.create_pool(**self.postgres_config)
                logger.info("âœ… PostgreSQL ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ PostgreSQL ì—°ê²° ì‹¤íŒ¨, PgVectorë§Œ ì‚¬ìš©: {e}")
                self.postgres_pool = None
            
            # Redis ì—°ê²°
            try:
                self.redis_client = redis.Redis(
                    host=os.getenv('REDIS_HOST', 'localhost'),
                    port=int(os.getenv('REDIS_PORT', 6379)),
                    decode_responses=True,
                    socket_connect_timeout=5
                )
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                self.redis_client.ping()
                logger.info("âœ… Redis ì—°ê²° ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨, ìºì‹œ ë¹„í™œì„±í™”: {e}")
                self.redis_client = None
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def close(self):
        """ì—°ê²° í’€ ì •ë¦¬"""
        if self.pgvector_pool:
            await self.pgvector_pool.close()
        if self.postgres_pool:
            await self.postgres_pool.close()
        if self.redis_client:
            self.redis_client.close()
    
    @asynccontextmanager
    async def get_pgvector_connection(self):
        """PgVector ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if not self.pgvector_pool:
            raise RuntimeError("PgVector ì—°ê²° í’€ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        conn = await self.pgvector_pool.acquire()
        try:
            yield conn
        finally:
            await self.pgvector_pool.release(conn)
    
    @asynccontextmanager
    async def get_postgres_connection(self):
        """PostgreSQL ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if not self.postgres_pool:
            # í´ë°±: PgVector ì‚¬ìš©
            async with self.get_pgvector_connection() as conn:
                yield conn
            return
                
        conn = await self.postgres_pool.acquire()
        try:
            yield conn
        finally:
            await self.postgres_pool.release(conn)
    
    def get_cache_key(self, prefix: str, params: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        params_str = json.dumps(params, sort_keys=True, default=str)
        hash_str = hashlib.md5(params_str.encode()).hexdigest()[:10]
        return f"{prefix}:{hash_str}"
    
    async def get_cached_result(self, cache_key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not self.redis_client:
            return None
        
        try:
            cached = self.redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None
    
    async def set_cached_result(self, cache_key: str, result: Any, ttl: int = None):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.redis_client:
            return
        
        try:
            ttl = ttl or self.cache_ttl
            self.redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result, default=str, ensure_ascii=False)
            )
        except Exception as e:
            logger.debug(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def search_news_advanced(
        self, 
        query: str, 
        limit: int = 10,
        use_vector_search: bool = True,
        similarity_threshold: float = 0.3
    ) -> List[NewsArticle]:
        """ê³ ê¸‰ ë‰´ìŠ¤ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self.get_cache_key("news_search", {
            "query": query, 
            "limit": limit,
            "use_vector": use_vector_search,
            "threshold": similarity_threshold
        })
        
        cached_result = await self.get_cached_result(cache_key)
        if cached_result:
            logger.debug(f"ìºì‹œì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {len(cached_result)}ê°œ")
            return [NewsArticle(**article) for article in cached_result]
        
        try:
            async with self.get_pgvector_connection() as conn:
                results = []
                
                if use_vector_search:
                    # ë²¡í„° ê²€ìƒ‰ ì‹œë„
                    try:
                        # OpenAI ì„ë² ë”© ìƒì„±
                        from openai import OpenAI
                        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                        
                        embedding_response = client.embeddings.create(
                            model="text-embedding-ada-002",
                            input=query
                        )
                        query_embedding = embedding_response.data[0].embedding
                        
                        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
                        vector_query = """
                            SELECT id, title, summary, url, source, published_date,
                                   1 - (embedding <=> $1::vector) as similarity_score
                            FROM crypto_news 
                            WHERE embedding IS NOT NULL
                            ORDER BY embedding <=> $1::vector
                            LIMIT $2
                        """
                        
                        rows = await conn.fetch(vector_query, query_embedding, limit)
                        
                        for row in rows:
                            if row['similarity_score'] >= similarity_threshold:
                                article = NewsArticle(
                                    id=row['id'],
                                    title=row['title'],
                                    summary=row['summary'] or "",
                                    url=row['url'],
                                    source=row['source'],
                                    published_date=row['published_date'],
                                    relevance_score=float(row['similarity_score'])
                                )
                                results.append(article)
                        
                        logger.info(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
                        
                    except Exception as vector_error:
                        logger.warning(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {vector_error}")
                        use_vector_search = False
                
                # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ë¶€ì¡± ì‹œ í‚¤ì›Œë“œ ê²€ìƒ‰
                if not use_vector_search or len(results) < limit // 2:
                    keyword_query = """
                        SELECT id, title, summary, url, source, published_date,
                               CASE 
                                   WHEN title ILIKE $1 THEN 0.9
                                   WHEN summary ILIKE $1 THEN 0.7
                                   WHEN title ILIKE $2 OR summary ILIKE $2 THEN 0.5
                                   ELSE 0.3
                               END as relevance_score
                        FROM crypto_news 
                        WHERE title ILIKE $2 OR summary ILIKE $2
                        ORDER BY relevance_score DESC, published_date DESC
                        LIMIT $3
                    """
                    
                    exact_pattern = f"%{query}%"
                    broad_pattern = f"%{' '.join(query.split()[:2])}%"
                    
                    keyword_rows = await conn.fetch(
                        keyword_query, 
                        exact_pattern, 
                        broad_pattern, 
                        limit - len(results)
                    )
                    
                    for row in keyword_rows:
                        # ì¤‘ë³µ ì œê±°
                        if not any(r.id == row['id'] for r in results):
                            article = NewsArticle(
                                id=row['id'],
                                title=row['title'],
                                summary=row['summary'] or "",
                                url=row['url'],
                                source=row['source'],
                                published_date=row['published_date'],
                                relevance_score=float(row['relevance_score'])
                            )
                            results.append(article)
                    
                    logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì¶”ê°€: ì´ {len(results)}ê°œ ê²°ê³¼")
                
                # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                results.sort(key=lambda x: x.relevance_score, reverse=True)
                
                # ìºì‹œì— ì €ì¥
                cache_data = [article.to_dict() for article in results]
                await self.set_cached_result(cache_key, cache_data)
                
                return results[:limit]
                
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_recent_news(self, hours: int = 24, limit: int = 10) -> List[NewsArticle]:
        """ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ"""
        cache_key = self.get_cache_key("recent_news", {"hours": hours, "limit": limit})
        
        cached_result = await self.get_cached_result(cache_key)
        if cached_result:
            return [NewsArticle(**article) for article in cached_result]
        
        try:
            async with self.get_pgvector_connection() as conn:
                since_time = datetime.now() - timedelta(hours=hours)
                
                query = """
                    SELECT id, title, summary, url, source, published_date
                    FROM crypto_news 
                    WHERE published_date >= $1
                    ORDER BY published_date DESC
                    LIMIT $2
                """
                
                rows = await conn.fetch(query, since_time, limit)
                
                results = []
                for row in rows:
                    article = NewsArticle(
                        id=row['id'],
                        title=row['title'],
                        summary=row['summary'] or "",
                        url=row['url'],
                        source=row['source'],
                        published_date=row['published_date'],
                        relevance_score=0.8
                    )
                    results.append(article)
                
                # ìºì‹œì— ì €ì¥
                cache_data = [article.to_dict() for article in results]
                await self.set_cached_result(cache_key, cache_data, ttl=120)  # 2ë¶„ ìºì‹œ
                
                return results
                
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        cache_key = "db_stats"
        cached_result = await self.get_cached_result(cache_key)
        if cached_result:
            return cached_result
        
        try:
            async with self.get_pgvector_connection() as conn:
                stats_query = """
                    SELECT 
                        COUNT(*) as total_news,
                        COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as news_with_embedding,
                        COUNT(CASE WHEN published_date >= NOW() - INTERVAL '24 hours' THEN 1 END) as news_24h,
                        COUNT(CASE WHEN published_date >= NOW() - INTERVAL '7 days' THEN 1 END) as news_7d
                    FROM crypto_news
                """
                
                row = await conn.fetchrow(stats_query)
                
                stats = {
                    'total_news': row['total_news'],
                    'news_with_embedding': row['news_with_embedding'],
                    'news_24h': row['news_24h'],
                    'news_7d': row['news_7d'],
                    'embedding_coverage': (row['news_with_embedding'] / max(row['total_news'], 1)) * 100,
                    'last_updated': datetime.now().isoformat()
                }
                
                await self.set_cached_result(cache_key, stats, ttl=600)  # 10ë¶„ ìºì‹œ
                return stats
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
db_manager = DatabaseManager()